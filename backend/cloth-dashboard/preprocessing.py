# -*- coding: utf-8 -*-
"""clothes.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1cc98Rk5jdJi65jG1wFX1y7I0CrcN1Skc
"""

import nltk as nltk
import matplotlib.pyplot as plt
import re

# chrome-extension://mcgbeeipkmelnpldkobichboakdfaeon/images/logo-vertical.svg
from nltk.tokenize import word_tokenize

nltk.download("punkt")
from nltk.corpus import stopwords

nltk.download("stopwords")
from nltk.stem.snowball import SnowballStemmer
import qalsadi.lemmatizer
from nltk import pos_tag

nltk.download("averaged_perceptron_tagger")

"""# Data Cleaning"""

punctuation_re = "[?؟!٪,،@#$%&*€+-£_~\“̯/=><.\۰):؛}{÷%(\"'ًٌٍَُِّْ٠-٩]"

arabic_diacritics = re.compile(
    """
                             ّ    | # Tashdid
                             َ    | # Fatha
                             ً    | # Tanwin Fath
                             ُ    | # Damma
                             ٌ    | # Tanwin Damm
                             ِ    | # Kasra
                             ٍ    | # Tanwin Kasr
                             ْ    | # Sukun
                             ـ     # Tatwil/Kashida
                         """,
    re.VERBOSE,
)

# remove stopwords
ArabicStopwords = set(stopwords.words("arabic")) - set(
    ["لا", "ما", "إلا", "ليس", "لن", "لم", "دون", "غير", "لست", "مب", "مش"]
)
# ArabicStopwords.update('جدا', 'الله', 'والله', 'فقط', 'صراح', 'انا', 'او', 'مو') لكن

stemmer = SnowballStemmer("arabic")


def Clean_Text(text):
    # remove emoji
    emoji_re = re.sub(
        r"[^0-9\u0600-\u06ff\u0750-\u077f\ufb50-\ufbc1\ufbd3-\ufd3f\ufd50-\ufd8f\ufd50-\ufd8f\ufe70-\ufefc\uFDF0-\uFDFD.0-9]+",
        " ",
        text,
    )
    # remove punct
    no_punc = re.sub(punctuation_re, " ", emoji_re)

    # remove duplicated letters
    # Unicode range for Arabic letters is 0600-06FF
    # \u = Unicode
    # \1+ = When it contains more than one character from the range
    # \1 = back reference to the last substring matching the r parenthetical in the regular expression
    no_duplicate = re.sub(r"([\u0600-\u06FF])\1+", r"\1", no_punc)

    # remove non arabic letters
    no_english = re.sub(r"[a-zA-Z?]", " ", no_duplicate)

    # remove diacritics
    no_diacritics = re.sub(arabic_diacritics, "", no_duplicate)

    # normalize
    a = re.sub("[إأآا]", "ا", no_diacritics)
    t = re.sub("ة", "ه", a)
    k = re.sub("گ", "ك", t)

    # tokenize
    # tokens = nltk.word_tokenize(normalized_text)
    tokens = nltk.word_tokenize(k)
    # tokens = nltk.word_tokenize(text)

    # remove stop words
    no_stopwords = [word for word in tokens if word not in ArabicStopwords]
    # Stemming
    stemmedWords = [stemmer.stem(word) for word in no_stopwords]

    return " ".join(stemmedWords)


def pos_tag_text(text):
    sentences = nltk.sent_tokenize(text)

    tagged_sentences = []
    for sentence in sentences:
        # Tokenize the sentence into words
        words = nltk.word_tokenize(sentence)

        # Perform POS tagging
        tagged_words = nltk.pos_tag(words)

        # Convert the tagged words into a sentence
        tagged_sentence = " ".join([f"{word}/{tag}" for word, tag in tagged_words])
        tagged_sentences.append(tagged_sentence)

    return " ".join(tagged_sentences)
